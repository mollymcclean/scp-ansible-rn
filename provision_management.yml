#########################################
# Perform the initial management  steps #
#########################################

- name: Add extra disks to Bastion and RKE nodes
  hosts: type_bastion, type_rke
  remote_user: "{{ remote_user }}"
  roles:
    - role: extra_disk
      extra_disk_target_dir: /var/lib/docker
      extra_disk_dependent_service: docker
      # NOTE: This needs to be run before the nodes are connected to AD
    - rke_user

- name: Create private certificates & configure authentication
  hosts: type_bastion, type_rke, type_harbor, type_redis, type_postgres, type_vault, type_nfs_server
  remote_user: "{{ remote_user }}"
  become: true
  roles:
    - update_hostname
    - geerlingguy.ntp
    - bdellegrazie.ca_certificates
    - role: ssh_ad_integration
      when: id_and_auth_service == 'active_directory'

- name: Create LetsEncrypt Certificates
  hosts: type_bastion
  remote_user: "{{ remote_user }}"
  roles:
    - role: lets_encrypt_cert
      lets_encrypt_cert_name: "harbor"
      lets_encrypt_r53_zone: "{{ r53_zone }}"
      lets_encrypt_domain: "{{ harbor_domain }}"
      lets_encrypt_certs_path: "{{ harbor_certs_path }}"
      lets_encrypt_acme_directory: "{{ acme_directory }}"
      when: harbor_lets_encrypt_cert

- hosts: type_nfs_server
  remote_user: "{{ remote_user }}"
  become: true
  roles:
    - extra_disk
    - nfs_server          

- name: Configure the redis servers (including sentinel)
  hosts: type_redis
  remote_user: "{{ remote_user }}"
  become: true
  gather_facts: yes
  roles:
    - sentinel_facts
    - role: redis_dependencies
      tags: ['online_install']

    # Delay local facts on the redis build - we want them generated by the sentinel build of ansible as they are all
    # on the same server. Otherwise we loose the information about the sentinel builds. Build the master node, then
    # the slaves so they can connect to the master (they require an extra variable defined too - which comes from the
    # previous redis_status module
    - role: davidwittman.redis
      local_facts: false
      when: ansible_default_ipv4.address == redis_master_node

    - role: davidwittman.redis
      redis_slaveof: "{{ redis_master_node }} 6379"
      local_facts: false
      when: ansible_default_ipv4.address != redis_master_node

    - role: davidwittman.redis
      redis_sentinel: True
      redis_sentinel_monitors:
        - name: "{{ redis_sentinal_cluster_name }}"
          host: "{{ redis_master_node }}"
          port: 6379
          quorum: 2
          auth_pass: "{{ redis_password }}"
          down_after_milliseconds: 15000
          parallel_syncs: 1
          failover_timeout: 180000
          notification_script: false
          client_reconfig_script: false

# Sanity check is in a separate play so that we can fully bail on failure conditions (e.g. even numbers of db hosts)
- hosts: type_postgres
  become: true
  remote_user: "{{ remote_user }}"
  any_errors_fatal: true
  roles:
    - role: extra_disk
      extra_disk_owner: postgres
      extra_disk_group: postgres
    - ansible-postgresql-cluster-ha-sanity

# Now we've validated we have a sane setup we group the rest of the postgres/patroni/consul setup together into a play.
# We choose to run everything on a single node. But this should work with hosts on separate nodes. See the original code
# that this was adapted from at https://github.com/v9258888900/ansible-postgresql-cluster-ha
- hosts: type_postgres
  become: true
  remote_user: "{{ remote_user }}"
  roles:
    - role: ansible-postgresql-cluster-ha-consul
      consul_group: type_postgres
    - ansible-postgresql-cluster-ha-postgres
    - ansible-postgresql-cluster-ha-patroni
    - ansible-postgresql-cluster-ha-haproxy
    - role: evrardjp.keepalived
      keepalived_scripts:
        postgres_status_check:
          check_script: "{{ postgres_check_script }}"
      keepalived_instances:
        redis_internal:
          interface: "{{ keepalived_instances_internal_interface }}"
          advert_int: 1
          state: BACKUP
          virtual_router_id: "{{ postgres_haproxy_virtual_router_id }}"
          priority: "{{ (play_hosts | length - play_hosts.index(inventory_hostname)) * 30 // (play_hosts|length) }}"
          nopreempt: true
          unicast_src_ip: "{{ hostvars[inventory_hostname]['guest.ipAddress' if rke_provider == 'vsphere' else 'private_ip_address'] }}"
          unicast_peers: "{{ play_hosts | map('extract', hostvars, 'guest.ipAddress' if rke_provider == 'vsphere' else 'private_ip_address') | list }}"
          track_scripts: 
            - postgres_status_check
          vips:
            - "{{ postgres_cluster_floating_ip }}"
    - ansible-postgresql-cluster-ha-consul-template

# Now provision all our required DBs
- hosts: type_postgres
  remote_user: "{{ remote_user }}"
  roles:
    - harbor_db_creation

- hosts: type_harbor
  remote_user: "{{ remote_user }}"
  roles:
    - role: extra_disk
      extra_disk_target_dir: /var/lib/docker
      extra_disk_dependent_service: docker

- hosts: type_harbor
  remote_user: "{{ remote_user }}"
  become: true
  roles:
    - role: evrardjp.keepalived
      keepalived_scripts:
        redis_status_check: 
          check_script: "{{ redis_check_script }}"
        harbor_status_check: 
          check_script: "{{ harbor_check_script }}" 
      keepalived_instances:
        redis_internal:
          interface: "{{ keepalived_instances_internal_interface }}"
          advert_int: 1
          state: BACKUP
          virtual_router_id: "{{ redis_cluster_virtual_router_id }}"
          priority: "{{ (play_hosts | length - play_hosts.index(inventory_hostname)) * 30 // (play_hosts|length) }}"
          nopreempt: true
          unicast_src_ip: "{{ hostvars[inventory_hostname]['guest.ipAddress' if rke_provider == 'vsphere' else 'private_ip_address'] }}"
          unicast_peers: "{{ play_hosts | map('extract', hostvars, 'guest.ipAddress' if rke_provider == 'vsphere' else 'private_ip_address') | list }}"
          track_scripts: 
            - redis_status_check
          vips:
            - "{{ redis_cluster_floating_ip }}"

        harbor_internal:
          interface: "{{ keepalived_instances_internal_interface }}"
          advert_int: 1
          state: BACKUP
          virtual_router_id: "{{ harbor_cluster_virtual_router_id }}"
          priority: "{{ (play_hosts | length - play_hosts.index(inventory_hostname)) * 30 // (play_hosts|length) }}"
          nopreempt: true
          unicast_src_ip: "{{ hostvars[inventory_hostname]['guest.ipAddress' if rke_provider == 'vsphere' else 'private_ip_address'] }}"
          unicast_peers: "{{ play_hosts | map('extract', hostvars, 'guest.ipAddress' if rke_provider == 'vsphere' else 'private_ip_address') | list }}"
          track_scripts: 
            - harbor_status_check
          vips:
            - "{{ harbor_cluster_floating_ip }}"
      when: groups['type_harbor'] | length > 1  

- hosts: type_harbor
  remote_user: "{{ remote_user }}"
  roles:
    - role: nfs_mount
      when: groups['type_harbor'] | length > 1
    - role: copy_harbor_certs
      when: harbor_self_signed_cert
    - role: cert_from_existing_ca
      cert_from_ca_cert_name: "harbor"
      cert_from_ca_ssl_root_dir: "{{ harbor_certs_path }}"
      cert_from_ca_ssl_common_name: "{{ harbor_hostname }}"
      when: harbor_ca_signed_cert_available
    - role: harbor
      # The test is having one or more postgres database servers
      harbor_external_database: "{{ groups['type_postgres'] is defined }}"
      harbor_external_redis: "{{ groups['type_redis'] is defined }}"


####################
# Final Management #
####################

- name: Transfer helm charts to the Bastion node
  hosts: type_bastion
  remote_user: "{{ remote_user }}"
  roles:
    - rancher_helm_chart_transfer

- name: Push Rancher container images to Harbor
  hosts: type_bastion
  remote_user: "{{ remote_user }}"
  roles:    
    - harbor_push_images

- name: Sync all server times to their clock source
  hosts: all
  remote_user: "{{ remote_user }}"
  roles: 
    - sync_time

- name: Prepare the RKE nodes
  hosts: type_rke
  remote_user: "{{ remote_user }}"
  roles:
    - rancher_k8s_node_hardening
    - rke

- name: Provision the RKE K8s cluster and deploy Rancher
  hosts: type_bastion
  remote_user: "{{ remote_user }}"
  roles:
    - role: rancher_helm_setup
      tags: ['online_install']
    - role: cert_from_existing_ca
      cert_from_ca_cert_name: rancher_ui
      cert_from_ca_ssl_root_dir: "{{ deploy_rancher_ui_tls_path }}"
      cert_from_ca_ssl_common_name: "{{ rancher_url }}"
      cert_from_ca_chain_cert_pem_dest: rancher-ca-chain.cert.pem
      when: deploy_rancher_ui_cert_type == 'custom-ca'
    - role: provision_rke
      rke_kubernetes_version: "{{ rancher_management_kubernetes_version }}"
    - deploy_rancher_ui

- name: Configure keepalived for the Rancher UI
  hosts: type_rke
  become: true
  remote_user: "{{ remote_user }}"
  roles:
    - role: evrardjp.keepalived
      keepalived_scripts:
        rancher_check_script:
          check_script: "{{ rancher_check_script }}"    
      keepalived_instances:
        rancher_internal:
          interface: "{{ keepalived_instances_internal_interface }}"
          advert_int: 1
          state: BACKUP
          virtual_router_id: "{{ rke_cluster_virtual_router_id }}"
          priority: "{{ (play_hosts | length - play_hosts.index(inventory_hostname)) * 30 // (play_hosts|length) }}"
          nopreempt: true
          unicast_src_ip: "{{ hostvars[inventory_hostname]['guest.ipAddress' if rke_provider == 'vsphere' else 'private_ip_address'] }}"
          unicast_peers: "{{ play_hosts | map('extract', hostvars, 'guest.ipAddress' if rke_provider == 'vsphere' else 'private_ip_address') | list }}"
          track_scripts: 
            - rancher_check_script
          vips:
            - "{{ rke_cluster_floating_ip }}"
      when: deploy_rke_keepalived

##############################################################
# Configure the Rancher admin user password, URL & telemetry #
##############################################################

- hosts: type_bastion
  remote_user: "{{ remote_user }}"
  roles:
    - role: configure_rancher_ui
      tags: configure_rancher_ui


#################################################################################
# Create cloud provider login, node templates, rke template and managed cluster #
#################################################################################

- name: Run the setup_rancher playbook
  import_playbook: setup_rancher.yml
  tags: setup_rancher
